# æœ¬åœ°GPUéƒ¨ç½²å®Œæ•´æŒ‡å—

**é€‚ç”¨åœºæ™¯**: åœ¨æœ‰GPUçš„æœºå™¨ï¼ˆå¦‚RTX 3070ï¼‰ä¸Šéƒ¨ç½²çœŸå®LLM

---

## âœ… ä½ çš„æµç¨‹æ˜¯å¯¹çš„ï¼

**æ­¥éª¤**: git clone â†’ ä¸‹è½½æ¨¡å‹ â†’ dockeréƒ¨ç½²

**è·¯å¾„ç¡®è®¤**: 
- âœ… æ¨¡å‹ä¸‹è½½è·¯å¾„: `./models/chatglm3-6b` (ç›¸å¯¹è·¯å¾„)
- âœ… Dockerè‡ªåŠ¨æŒ‚è½½: `./models` â†’ `/app/models` (å®¹å™¨å†…)
- âœ… é…ç½®æ–‡ä»¶è·¯å¾„: `./models/chatglm3-6b` (ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•)

---

## ğŸ“‹ å®Œæ•´éƒ¨ç½²æ­¥éª¤ï¼ˆ3070æœºå™¨ï¼‰

### æ­¥éª¤1ï¼šè·å–ä»£ç 

```bash
# å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡
git clone https://github.com/conan7072/stock-agent.git
cd stock-agent

# å¦‚æœå·²ç»cloneè¿‡ï¼Œæ›´æ–°åˆ°æœ€æ–°ç‰ˆ
cd stock-agent
git pull origin main
```

**éªŒè¯**:
```bash
ls -la
# åº”è¯¥çœ‹åˆ°ï¼šserver/ client/ scripts/ data/ Dockerfile docker-compose.yml ç­‰
```

---

### æ­¥éª¤2ï¼šå®‰è£…ä¸‹è½½ä¾èµ–

```bash
# å®‰è£…huggingface_hubï¼ˆä¸‹è½½æ¨¡å‹å¿…éœ€ï¼‰
pip install huggingface_hub

# æˆ–å®‰è£…å®Œæ•´ä¾èµ–
pip install -r requirements.txt
```

### æ­¥éª¤3ï¼šä¸‹è½½æ¨¡å‹

```bash
# ä¸‹è½½ChatGLM3-6B INT4ï¼ˆé€‚åˆRTX 3070 8GBï¼‰
python scripts/download_model.py --model chatglm3-6b-int4
```

**é¢„æœŸè¾“å‡º**:
```
======================================================================
æ¨¡å‹ä¸‹è½½å·¥å…· - chatglm3-6b-int4
======================================================================

æ¨¡å‹: chatglm3-6b-int4
æè¿°: ChatGLM3-6B INT4é‡åŒ–ç‰ˆï¼ˆæ¨èRTX 3070ï¼‰
æ–‡ä»¶å¤§å°: ~13GB
æ˜¾å­˜éœ€æ±‚: 4-5GB
ä¿å­˜è·¯å¾„: ./models/chatglm3-6b
Hugging Face: THUDM/chatglm3-6b

[æ­¥éª¤ 1/5] æ£€æŸ¥ä¾èµ–...
âœ“ huggingface_hub å·²å®‰è£…

[æ­¥éª¤ 2/5] æ£€æŸ¥ç£ç›˜ç©ºé—´...
å½“å‰ç›®å½•å¯ç”¨ç©ºé—´: 125.3 GB
âœ“ ç£ç›˜ç©ºé—´å……è¶³

[æ­¥éª¤ 3/5] æ£€æŸ¥ç°æœ‰æ–‡ä»¶...
âœ“ ç›®å½•ä¸ºç©ºï¼Œå‡†å¤‡ä¸‹è½½

[æ­¥éª¤ 4/5] é…ç½®ä¸‹è½½...
âœ“ ä½¿ç”¨é•œåƒ: https://hf-mirror.com

[æ­¥éª¤ 5/5] å¼€å§‹ä¸‹è½½...
æ­£åœ¨ä» Hugging Face ä¸‹è½½...
ä»“åº“: THUDM/chatglm3-6b

... (ä¸‹è½½è¿‡ç¨‹ï¼Œå¯èƒ½éœ€è¦10-30åˆ†é’Ÿ)

======================================================================
âœ“ ä¸‹è½½å®Œæˆï¼
======================================================================

è€—æ—¶: 15.3 åˆ†é’Ÿ
è·¯å¾„: ./models/chatglm3-6b
```

**éªŒè¯**:
```bash
ls -la models/chatglm3-6b/
# åº”è¯¥çœ‹åˆ°æ¨¡å‹æ–‡ä»¶ï¼ˆconfig.json, pytorch_model.binç­‰ï¼‰
```

---

### æ­¥éª¤4ï¼šä¿®æ”¹é…ç½®

ç¼–è¾‘ `server/configs/server_config.yaml`ï¼š

```yaml
model:
  mock_mode: false              # â† æ”¹ä¸º falseï¼ˆé‡è¦ï¼ï¼‰
  name: chatglm3-6b
  path: ./models/chatglm3-6b    # â† ç›¸å¯¹è·¯å¾„ï¼ŒDockerä¼šè‡ªåŠ¨æŒ‚è½½
  device: cuda                  # â† ä½¿ç”¨GPU
  quantization: int4
  max_length: 4096
  temperature: 0.7
  top_p: 0.9

server:
  host: 0.0.0.0
  port: 8765
```

**éªŒè¯**:
```bash
cat server/configs/server_config.yaml | grep mock_mode
# åº”è¯¥æ˜¾ç¤º: mock_mode: false
```

---

### æ­¥éª¤5ï¼šDockeréƒ¨ç½²

```bash
# å¯åŠ¨GPUç‰ˆæœ¬ï¼ˆChatGLM3ï¼‰
docker-compose --profile chatglm3 up -d
```

**é¢„æœŸè¾“å‡º**:
```
[+] Running 1/1
 âœ” Container stock-agent-chatglm3  Started
```

**æŸ¥çœ‹æ—¥å¿—**:
```bash
docker-compose logs -f agent-chatglm3
```

**é¢„æœŸæ—¥å¿—**:
```
============================================================
ğŸš€ å¯åŠ¨è‚¡ç¥¨å’¨è¯¢AgentæœåŠ¡...
============================================================
æ¨¡å¼: gpu
ç«¯å£: 8765
æ—¶é—´: Thu Jan 22 12:34:56 UTC 2026
============================================================

INFO:     Started server process [1]
INFO:     Waiting for application startup.
============================================================
å¯åŠ¨è‚¡ç¥¨å’¨è¯¢AgentæœåŠ¡...
============================================================
æ­£åœ¨åŠ è½½æ¨¡å‹: ./models/chatglm3-6b
å·²åŠ è½½ 21 æ¡çŸ¥è¯†åº“ç´¢å¼•
Agentåˆå§‹åŒ–å®Œæˆï¼šLLM=ChatGLM3LLM, å·¥å…·æ•°=5

æœåŠ¡å·²å¯åŠ¨:
  - Host: 0.0.0.0
  - Port: 8765
  - API Docs: http://0.0.0.0:8765/docs
============================================================
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8765 (Press CTRL+C to quit)
```

---

### æ­¥éª¤6ï¼šæµ‹è¯•æœåŠ¡

**æµ‹è¯•å¥åº·æ£€æŸ¥**:
```bash
curl http://localhost:8765/health
```

**é¢„æœŸè¾“å‡º**:
```json
{
  "status": "healthy",
  "agent_ready": true
}
```

**æµ‹è¯•èŠå¤©**:
```bash
curl -X POST http://localhost:8765/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "æ¯”äºšè¿ªç°åœ¨å¤šå°‘é’±ï¼Ÿ"}'
```

**æˆ–ä½¿ç”¨æµè§ˆå™¨**:
è®¿é—® http://localhost:8765/docs è¿›è¡Œäº¤äº’å¼æµ‹è¯•

---

## ğŸ“‚ è·¯å¾„è¯´æ˜

### é¡¹ç›®ç»“æ„
```
stock-agent/                    (é¡¹ç›®æ ¹ç›®å½•)
â”œâ”€â”€ models/                     â† æ¨¡å‹å­˜æ”¾ï¼ˆä½ ä¸‹è½½çš„ï¼‰
â”‚   â””â”€â”€ chatglm3-6b/           
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â””â”€â”€ tokenizer_config.json
â”œâ”€â”€ data/                       â† æ•°æ®æ–‡ä»¶
â”‚   â”œâ”€â”€ stocks/                
â”‚   â””â”€â”€ knowledge/
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ server_config.yaml  â† é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ src/
â”œâ”€â”€ Dockerfile
â””â”€â”€ docker-compose.yml
```

### DockeræŒ‚è½½é…ç½®

åœ¨ `docker-compose.yml` ä¸­ï¼ˆGPUç‰ˆæœ¬ï¼‰ï¼š

```yaml
volumes:
  - ./data:/app/data              # æ•°æ®ç›®å½•
  - ./models:/app/models          # â† æ¨¡å‹ç›®å½•ï¼ˆæŒ‚è½½åˆ°å®¹å™¨ï¼‰
  - ./server/configs:/app/server/configs  # é…ç½®ç›®å½•
  - ./logs:/app/logs              # æ—¥å¿—ç›®å½•
```

**å·¥ä½œåŸç†**:
1. ä½ åœ¨ä¸»æœºä¸‹è½½æ¨¡å‹åˆ°: `./models/chatglm3-6b`
2. Dockerå¯åŠ¨æ—¶è‡ªåŠ¨æŒ‚è½½: `./models` â†’ `/app/models`
3. å®¹å™¨å†…è®¿é—®è·¯å¾„: `/app/models/chatglm3-6b`
4. é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨ç›¸å¯¹è·¯å¾„: `./models/chatglm3-6b`
5. å®¹å™¨å†…è‡ªåŠ¨è§£æä¸º: `/app/models/chatglm3-6b` âœ“

**æ‰€ä»¥ä½ çš„æµç¨‹å®Œå…¨æ­£ç¡®ï¼** âœ…

---

## ğŸ” éªŒè¯è·¯å¾„æ˜¯å¦æ­£ç¡®

### æ–¹æ³•1ï¼šæ£€æŸ¥é…ç½®
```bash
# ä¸»æœºä¸Š
cat server/configs/server_config.yaml | grep path
# è¾“å‡º: path: ./models/chatglm3-6b
```

### æ–¹æ³•2ï¼šè¿›å…¥å®¹å™¨æ£€æŸ¥
```bash
# è¿›å…¥è¿è¡Œä¸­çš„å®¹å™¨
docker exec -it stock-agent-chatglm3 bash

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -la /app/models/chatglm3-6b/

# åº”è¯¥çœ‹åˆ°æ¨¡å‹æ–‡ä»¶
# config.json
# pytorch_model.bin
# ...

# é€€å‡ºå®¹å™¨
exit
```

### æ–¹æ³•3ï¼šæŸ¥çœ‹æ—¥å¿—
```bash
docker-compose logs agent-chatglm3 | grep "æ¨¡å‹"

# åº”è¯¥çœ‹åˆ°ç±»ä¼¼ï¼š
# æ­£åœ¨åŠ è½½æ¨¡å‹: ./models/chatglm3-6b
# æ¨¡å‹åŠ è½½æˆåŠŸ
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: æ¨¡å‹ä¸‹è½½å¤±è´¥

**é”™è¯¯**: `ç½‘ç»œè¿æ¥è¶…æ—¶` æˆ– `ä¸‹è½½ä¸­æ–­`

**è§£å†³**:
```bash
# ä¸‹è½½è„šæœ¬æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œç›´æ¥é‡æ–°è¿è¡Œå³å¯
python scripts/download_model.py --model chatglm3-6b-int4
```

### Q2: Dockeræ‰¾ä¸åˆ°æ¨¡å‹

**é”™è¯¯**: å®¹å™¨æ—¥å¿—æ˜¾ç¤º `FileNotFoundError: models/chatglm3-6b`

**æ£€æŸ¥**:
```bash
# 1. ç¡®è®¤æ¨¡å‹å·²ä¸‹è½½
ls -la models/chatglm3-6b/

# 2. ç¡®è®¤docker-compose.ymlæœ‰volumesé…ç½®
grep -A2 "volumes:" docker-compose.yml

# 3. ç¡®è®¤ä½¿ç”¨äº†æ­£ç¡®çš„profile
docker-compose --profile chatglm3 up -d  # â† å¿…é¡»æŒ‡å®šprofile
```

### Q3: GPUæœªè¢«ä½¿ç”¨

**é”™è¯¯**: å®¹å™¨æ—¥å¿—æ˜¾ç¤ºä½¿ç”¨CPU

**æ£€æŸ¥**:
```bash
# 1. ç¡®è®¤NVIDIA Dockerå·²å®‰è£…
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# 2. ç¡®è®¤é…ç½®æ–‡ä»¶ä¸­ device: cuda
cat server/configs/server_config.yaml | grep device

# 3. ç¡®è®¤docker-compose.ymlæœ‰GPUé…ç½®
grep -A5 "deploy:" docker-compose.yml
```

### Q4: ç£ç›˜ç©ºé—´ä¸è¶³

**é”™è¯¯**: ä¸‹è½½ä¸­é€”å¤±è´¥ï¼Œæç¤ºç©ºé—´ä¸è¶³

**è§£å†³**:
```bash
# æ¸…ç†Dockeré•œåƒé‡Šæ”¾ç©ºé—´
docker system prune -a

# æˆ–ä½¿ç”¨æ›´å¤§çš„ç£ç›˜
# ä¿®æ”¹ download_model.py ä¸­çš„ model_info['path'] ä¸ºå…¶ä»–è·¯å¾„
```

---

## ğŸ¯ å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```bash
# 1. å…‹éš†/æ›´æ–°ä»£ç 
git clone https://github.com/conan7072/stock-agent.git
cd stock-agent
# æˆ–
git pull origin main

# 2. ä¸‹è½½æ¨¡å‹
python scripts/download_model.py --model chatglm3-6b-int4

# 3. ä¿®æ”¹é…ç½®
vim server/configs/server_config.yaml
# è®¾ç½® mock_mode: false

# 4. å¯åŠ¨æœåŠ¡
docker-compose --profile chatglm3 up -d

# 5. æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f agent-chatglm3

# 6. æµ‹è¯•æœåŠ¡
curl http://localhost:8765/health

# 7. åœæ­¢æœåŠ¡
docker-compose --profile chatglm3 down
```

---

## ğŸ“Š æ€§èƒ½é¢„æœŸï¼ˆRTX 3070 8GBï¼‰

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| **æ˜¾å­˜å ç”¨** | 4-5GB |
| **é¦–æ¬¡å“åº”** | 1-2ç§’ |
| **ç”Ÿæˆé€Ÿåº¦** | 15-20 tokens/s |
| **å¹¶å‘æ”¯æŒ** | 2-3äºº |
| **æ¨ç†è´¨é‡** | ä¼˜ç§€ |

---

## âœ… æ€»ç»“

**ä½ çš„æµç¨‹å®Œå…¨æ­£ç¡®**ï¼š

1. âœ… `git clone` è·å–ä»£ç 
2. âœ… `python scripts/download_model.py` ä¸‹è½½æ¨¡å‹åˆ° `./models/`
3. âœ… ä¿®æ”¹é…ç½® `mock_mode: false`
4. âœ… `docker-compose --profile chatglm3 up -d` å¯åŠ¨
5. âœ… Dockerè‡ªåŠ¨æŒ‚è½½ `./models` â†’ `/app/models`
6. âœ… å®¹å™¨å†…æ­£ç¡®è¯»å–æ¨¡å‹ âœ“

**è·¯å¾„æ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒDockerèƒ½è¯»åˆ°ï¼** âœ…

---

**å¼€å§‹åœ¨ä½ çš„3070æœºå™¨ä¸Šéƒ¨ç½²å§ï¼** ğŸš€
