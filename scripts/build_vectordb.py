"""
å‘é‡æ•°æ®åº“æ„å»ºè„šæœ¬

è¯»å– data/knowledge/ ç›®å½•ä¸‹çš„æ‰€æœ‰markdownæ–‡ä»¶ï¼Œ
ä½¿ç”¨ bge-small-zh-v1.5 è¿›è¡Œå‘é‡åŒ–ï¼Œ
å­˜å‚¨åˆ° Chroma å‘é‡æ•°æ®åº“
"""

import sys
from pathlib import Path
from typing import List

def load_documents():
    """åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£"""
    knowledge_dir = Path(__file__).parent.parent / "data" / "knowledge"
    
    if not knowledge_dir.exists():
        print(f"âŒ çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {knowledge_dir}")
        return []
    
    # æŸ¥æ‰¾æ‰€æœ‰markdownæ–‡ä»¶
    md_files = list(knowledge_dir.rglob("*.md"))
    
    if not md_files:
        print(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½•markdownæ–‡ä»¶")
        return []
    
    print(f"ğŸ“š æ‰¾åˆ° {len(md_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
    
    documents = []
    
    for file_path in md_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–æ–‡ä»¶ä¿¡æ¯
            relative_path = file_path.relative_to(knowledge_dir)
            category = relative_path.parts[0] if len(relative_path.parts) > 1 else "general"
            
            documents.append({
                "content": content,
                "metadata": {
                    "source": str(relative_path),
                    "category": category,
                    "filename": file_path.name
                }
            })
            
            print(f"   âœ… {relative_path}")
            
        except Exception as e:
            print(f"   âŒ {file_path.name}: {e}")
    
    return documents

def split_documents(documents: List[dict], chunk_size: int = 500, chunk_overlap: int = 50):
    """åˆ†å‰²æ–‡æ¡£ä¸ºå°å—"""
    try:
        from langchain.text_splitter import RecursiveCharacterTextSplitter
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£… langchain:")
        print("   pip install langchain")
        sys.exit(1)
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]
    )
    
    chunks = []
    
    for doc in documents:
        texts = text_splitter.split_text(doc["content"])
        
        for i, text in enumerate(texts):
            chunks.append({
                "content": text,
                "metadata": {
                    **doc["metadata"],
                    "chunk_id": i,
                    "total_chunks": len(texts)
                }
            })
    
    print(f"ğŸ“„ æ–‡æ¡£åˆ†å‰²å®Œæˆ: {len(chunks)} ä¸ªå—")
    
    return chunks

def build_vectordb(chunks: List[dict]):
    """æ„å»ºå‘é‡æ•°æ®åº“"""
    try:
        from langchain_community.embeddings import HuggingFaceEmbeddings
        from langchain_community.vectorstores import Chroma
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£…ä¾èµ–:")
        print("   pip install langchain-community sentence-transformers chromadb")
        sys.exit(1)
    
    # å‘é‡æ•°æ®åº“ä¿å­˜è·¯å¾„
    vectordb_dir = Path(__file__).parent.parent / "data" / "vector_db"
    vectordb_dir.mkdir(parents=True, exist_ok=True)
    
    print()
    print("ğŸ”§ åˆå§‹åŒ–å‘é‡æ¨¡å‹: bge-small-zh-v1.5")
    print("   é¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦90MBï¼‰...")
    
    # åˆ›å»º embedding æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-small-zh-v1.5",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    print("   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print()
    print("ğŸ”„ å‘é‡åŒ–æ–‡æ¡£...")
    
    # å‡†å¤‡æ•°æ®
    texts = [chunk["content"] for chunk in chunks]
    metadatas = [chunk["metadata"] for chunk in chunks]
    
    # åˆ›å»ºå‘é‡æ•°æ®åº“
    vectordb = Chroma.from_texts(
        texts=texts,
        embedding=embeddings,
        metadatas=metadatas,
        persist_directory=str(vectordb_dir)
    )
    
    # æŒä¹…åŒ–
    vectordb.persist()
    
    print("   âœ… å‘é‡åŒ–å®Œæˆ")
    
    return vectordb, vectordb_dir

def test_retrieval(vectordb):
    """æµ‹è¯•æ£€ç´¢åŠŸèƒ½"""
    print()
    print("ğŸ§ª æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
    print()
    
    test_queries = [
        "ä»€ä¹ˆæ˜¯è‚¡ç¥¨ï¼Ÿ",
        "å¦‚ä½•è®¡ç®—å¸‚ç›ˆç‡ï¼Ÿ",
        "MACDæŒ‡æ ‡æ€ä¹ˆä½¿ç”¨ï¼Ÿ"
    ]
    
    for query in test_queries:
        print(f"æŸ¥è¯¢: {query}")
        results = vectordb.similarity_search(query, k=2)
        
        for i, doc in enumerate(results, 1):
            print(f"   [{i}] æ¥æº: {doc.metadata.get('source', 'unknown')}")
            preview = doc.page_content[:100].replace('\n', ' ')
            print(f"       å†…å®¹: {preview}...")
        print()

def main():
    """ä¸»å‡½æ•°"""
    print()
    print("=" * 60)
    print("ğŸ”§ å‘é‡æ•°æ®åº“æ„å»ºå·¥å…·")
    print("=" * 60)
    print()
    
    # 1. åŠ è½½æ–‡æ¡£
    print("ğŸ“š æ­¥éª¤1: åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£")
    documents = load_documents()
    
    if not documents:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£")
        sys.exit(1)
    
    print()
    
    # 2. åˆ†å‰²æ–‡æ¡£
    print("ğŸ“„ æ­¥éª¤2: åˆ†å‰²æ–‡æ¡£")
    chunks = split_documents(documents, chunk_size=500, chunk_overlap=50)
    print()
    
    # 3. æ„å»ºå‘é‡æ•°æ®åº“
    print("ğŸ”§ æ­¥éª¤3: æ„å»ºå‘é‡æ•°æ®åº“")
    vectordb, vectordb_dir = build_vectordb(chunks)
    
    # 4. æµ‹è¯•æ£€ç´¢
    test_retrieval(vectordb)
    
    print("=" * 60)
    print("âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ ä½ç½®: {vectordb_dir}")
    print(f"ğŸ“Š æ–‡æ¡£æ•°: {len(chunks)} ä¸ªå—")
    
    # è®¡ç®—å¤§å°
    total_size = sum(f.stat().st_size for f in vectordb_dir.rglob('*') if f.is_file())
    print(f"ğŸ’¾ å¤§å°: {total_size / 1024 / 1024:.1f} MB")
    
    print()
    print("ğŸ”§ ä¸‹ä¸€æ­¥:")
    print("  1. å¯åŠ¨æœåŠ¡:")
    print("     python server/start_server.py")
    print()

if __name__ == "__main__":
    main()
