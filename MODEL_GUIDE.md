# æ¨¡å‹é€‰æ‹©æŒ‡å—

**ç¡¬ä»¶å‚è€ƒ**ï¼šRTX 3070 8GBæ˜¾å­˜

---

## ğŸ¯ æ¨èæ¨¡å‹ï¼ˆæŒ‰æ˜¾å­˜éœ€æ±‚ï¼‰

### 1. ChatGLM3-6B INT4 â­ æœ€æ¨è

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **æ˜¾å­˜éœ€æ±‚** | 4-5GB |
| **æ¨¡å‹æ–‡ä»¶** | ~13GB |
| **é€Ÿåº¦** | 15-20 tokens/s |
| **è´¨é‡** | ä¼˜ç§€ |
| **å¹¶å‘** | æ”¯æŒ2-3äºº |
| **é€‚åˆæ˜¾å¡** | RTX 3070 8GB âœ… |

**ä¸‹è½½å‘½ä»¤**ï¼š
```bash
python scripts/download_model.py --model chatglm3-6b-int4
```

**é…ç½®**ï¼š
```yaml
model:
  name: chatglm3-6b
  path: ./models/chatglm3-6b
  quantization: int4
  device: cuda
  max_length: 4096
```

---

### 2. Qwen2-7B INT4

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **æ˜¾å­˜éœ€æ±‚** | 5-6GB |
| **æ¨¡å‹æ–‡ä»¶** | ~14GB |
| **é€Ÿåº¦** | 12-18 tokens/s |
| **è´¨é‡** | ä¼˜ç§€ |
| **å¹¶å‘** | æ”¯æŒ1-2äºº |
| **é€‚åˆæ˜¾å¡** | RTX 3070 8GB âœ… |

**ä¸‹è½½å‘½ä»¤**ï¼š
```bash
python scripts/download_model.py --model qwen2-7b-int4
```

**é…ç½®**ï¼š
```yaml
model:
  name: qwen2-7b
  path: ./models/qwen2-7b
  quantization: int4
  device: cuda
  max_length: 4096
```

---

### 3. Qwen2-1.5B FP16 ğŸš€ æœ€å¿«

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **æ˜¾å­˜éœ€æ±‚** | 2-3GB |
| **æ¨¡å‹æ–‡ä»¶** | ~3GB |
| **é€Ÿåº¦** | 30-40 tokens/s |
| **è´¨é‡** | è‰¯å¥½ |
| **å¹¶å‘** | æ”¯æŒ4-5äºº |
| **é€‚åˆæ˜¾å¡** | RTX 3070 8GB âœ…âœ… |

**ä¸‹è½½å‘½ä»¤**ï¼š
```bash
python scripts/download_model.py --model qwen2-1.5b
```

**é…ç½®**ï¼š
```yaml
model:
  name: qwen2-1.5b
  path: ./models/qwen2-1.5b
  quantization: fp16
  device: cuda
  max_length: 4096
```

---

### 4. ChatGLM3-6B INT8

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **æ˜¾å­˜éœ€æ±‚** | 6-7GB |
| **æ¨¡å‹æ–‡ä»¶** | ~6.5GB |
| **é€Ÿåº¦** | 10-15 tokens/s |
| **è´¨é‡** | éå¸¸ä¼˜ç§€ |
| **å¹¶å‘** | æ”¯æŒ1äºº |
| **é€‚åˆæ˜¾å¡** | RTX 3070 8GB âš ï¸ ç´§å¼  |

**ä¸‹è½½å‘½ä»¤**ï¼š
```bash
python scripts/download_model.py --model chatglm3-6b-int8
```

**é…ç½®**ï¼š
```yaml
model:
  name: chatglm3-6b
  path: ./models/chatglm3-6b
  quantization: int8
  device: cuda
  max_length: 4096
```

---

### 5. Mock LLM ğŸ’» æ— GPU

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **æ˜¾å­˜éœ€æ±‚** | 0GB |
| **æ¨¡å‹æ–‡ä»¶** | 0GB |
| **é€Ÿåº¦** | å³æ—¶ |
| **è´¨é‡** | æ¨¡æ‹Ÿï¼ˆæ¨¡æ¿å“åº”ï¼‰ |
| **å¹¶å‘** | æ— é™åˆ¶ |
| **é€‚åˆåœºæ™¯** | å¼€å‘æµ‹è¯• |

**é…ç½®**ï¼š
```yaml
model:
  mock_mode: true
  name: mock
```

---

## ğŸ“Š æ˜¾å­˜éœ€æ±‚è¯¦è§£

### RTX 3070 8GB æ˜¾å­˜åˆ†é…

```
æ€»æ˜¾å­˜: 8GB
â”œâ”€â”€ ç³»ç»Ÿå ç”¨: 0.5-1GB
â”œâ”€â”€ æ¨¡å‹åŠ è½½: 4-5GB (INT4)
â”œâ”€â”€ æ¨ç†ç¼“å­˜: 1-2GB
â””â”€â”€ å‰©ä½™å¯ç”¨: 1-1.5GB
```

### ä¸åŒé‡åŒ–æ–¹å¼å¯¹æ¯”

| é‡åŒ–æ–¹å¼ | æ˜¾å­˜å ç”¨ | è´¨é‡æŸå¤± | é€Ÿåº¦ |
|---------|---------|---------|------|
| **FP16** | 12GB | 0% | åŸºå‡† |
| **INT8** | 6-7GB | ~2% | +20% |
| **INT4** | 4-5GB | ~5% | +40% |
| **INT2** | 2-3GB | ~15% | +60% |

---

## ğŸ¯ æ¨èé…ç½®ï¼ˆRTX 3070 8GBï¼‰

### åœºæ™¯1ï¼šè¿½æ±‚è´¨é‡ï¼ˆå•ç”¨æˆ·ï¼‰

```yaml
model:
  name: chatglm3-6b
  quantization: int4
  max_length: 4096
  concurrent_users: 1
```

**é¢„æœŸæ€§èƒ½**ï¼š
- å“åº”é€Ÿåº¦ï¼š15-20 tokens/s
- è´¨é‡ï¼šä¼˜ç§€
- å¹¶å‘ï¼š1äºº

---

### åœºæ™¯2ï¼šå¹³è¡¡æ€§èƒ½ï¼ˆ2-3ç”¨æˆ·ï¼‰

```yaml
model:
  name: chatglm3-6b
  quantization: int4
  max_length: 2048  # å‡å°‘é•¿åº¦
  concurrent_users: 2
```

**é¢„æœŸæ€§èƒ½**ï¼š
- å“åº”é€Ÿåº¦ï¼š12-18 tokens/s
- è´¨é‡ï¼šä¼˜ç§€
- å¹¶å‘ï¼š2-3äºº

---

### åœºæ™¯3ï¼šè¿½æ±‚é€Ÿåº¦ï¼ˆå¤šç”¨æˆ·ï¼‰

```yaml
model:
  name: qwen2-1.5b
  quantization: fp16
  max_length: 4096
  concurrent_users: 4
```

**é¢„æœŸæ€§èƒ½**ï¼š
- å“åº”é€Ÿåº¦ï¼š30-40 tokens/s
- è´¨é‡ï¼šè‰¯å¥½
- å¹¶å‘ï¼š4-5äºº

---

## ğŸ”§ é…ç½®æ–¹æ³•

### æ–¹å¼1ï¼šä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `server/configs/server_config.yaml`ï¼š

```yaml
model:
  mock_mode: false      # ä½¿ç”¨çœŸå®æ¨¡å‹
  name: chatglm3-6b     # æ¨¡å‹åç§°
  path: ./models/chatglm3-6b  # æ¨¡å‹è·¯å¾„
  device: cuda          # ä½¿ç”¨GPU
  quantization: int4    # é‡åŒ–æ–¹å¼
  max_length: 4096      # æœ€å¤§é•¿åº¦
  temperature: 0.7      # æ¸©åº¦
  top_p: 0.9           # Top-pé‡‡æ ·
```

### æ–¹å¼2ï¼šç¯å¢ƒå˜é‡

```bash
export MODEL_NAME=chatglm3-6b
export MODEL_QUANTIZATION=int4
export MODEL_MAX_LENGTH=4096

python start_server.py
```

---

## ğŸ“¥ æ¨¡å‹ä¸‹è½½

### ä½¿ç”¨ä¸‹è½½è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# ChatGLM3-6B INT4ï¼ˆæ¨èï¼‰
python scripts/download_model.py --model chatglm3-6b-int4

# Qwen2-7B INT4
python scripts/download_model.py --model qwen2-7b-int4

# Qwen2-1.5Bï¼ˆæœ€å¿«ï¼‰
python scripts/download_model.py --model qwen2-1.5b
```

### æ‰‹åŠ¨ä¸‹è½½

ä»Hugging Faceæˆ–ModelScopeä¸‹è½½ï¼š

```bash
# ä½¿ç”¨å›½å†…é•œåƒï¼ˆæ›´å¿«ï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# ChatGLM3-6B
git clone https://hf-mirror.com/THUDM/chatglm3-6b ./models/chatglm3-6b

# Qwen2-7B
git clone https://hf-mirror.com/Qwen/Qwen2-7B-Instruct ./models/qwen2-7b

# Qwen2-1.5B
git clone https://hf-mirror.com/Qwen/Qwen2-1.5B-Instruct ./models/qwen2-1.5b
```

---

## âš™ï¸ æ€§èƒ½ä¼˜åŒ–

### 1. å‡å°‘max_length

```yaml
max_length: 2048  # ä»4096å‡åˆ°2048
```

**æ•ˆæœ**ï¼šèŠ‚çœ1-2GBæ˜¾å­˜ï¼Œæ”¯æŒæ›´å¤šå¹¶å‘

### 2. å¯ç”¨Flash Attention

```yaml
use_flash_attention: true
```

**æ•ˆæœ**ï¼šé€Ÿåº¦æå‡20-30%ï¼Œæ˜¾å­˜å‡å°‘10-15%

### 3. è°ƒæ•´batch size

```yaml
batch_size: 1  # å•ç”¨æˆ·
# batch_size: 2  # 2-3ç”¨æˆ·
```

### 4. é™åˆ¶å¹¶å‘æ•°

```python
# server/src/api/main.py
MAX_CONCURRENT_REQUESTS = 2  # RTX 3070 8GBå»ºè®®2-3
```

---

## ğŸ§ª æµ‹è¯•æ€§èƒ½

### æµ‹è¯•è„šæœ¬

```bash
# æµ‹è¯•ç”Ÿæˆé€Ÿåº¦
python scripts/benchmark_model.py --model chatglm3-6b

# æµ‹è¯•æ˜¾å­˜å ç”¨
python scripts/check_vram.py

# å‹åŠ›æµ‹è¯•
python scripts/stress_test.py --users 3 --duration 300
```

### æ€§èƒ½åŸºå‡†ï¼ˆRTX 3070 8GBï¼‰

| æ¨¡å‹ | é¦–Token | ç”Ÿæˆé€Ÿåº¦ | æ˜¾å­˜ | å¹¶å‘ |
|------|--------|---------|------|------|
| ChatGLM3-6B INT4 | 1.2s | 18 t/s | 4.5GB | 2-3 |
| Qwen2-7B INT4 | 1.5s | 15 t/s | 5.5GB | 1-2 |
| Qwen2-1.5B FP16 | 0.5s | 35 t/s | 2.5GB | 4-5 |

---

## ğŸ¯ æœ€ç»ˆæ¨èï¼ˆRTX 3070 8GBï¼‰

### é¦–é€‰ï¼šChatGLM3-6B INT4

**åŸå› **ï¼š
- âœ… æ˜¾å­˜å ç”¨åˆç†ï¼ˆ4-5GBï¼‰
- âœ… è´¨é‡ä¼˜ç§€
- âœ… é€Ÿåº¦è¶³å¤Ÿå¿«ï¼ˆ15-20 t/sï¼‰
- âœ… æ”¯æŒ2-3äººå¹¶å‘
- âœ… ä¸­æ–‡æ•ˆæœå¥½

**é…ç½®**ï¼š
```yaml
model:
  mock_mode: false
  name: chatglm3-6b
  path: ./models/chatglm3-6b
  device: cuda
  quantization: int4
  max_length: 4096
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: 13GBæ¨¡å‹æ–‡ä»¶ä¸ºä»€ä¹ˆåªéœ€è¦4GBæ˜¾å­˜ï¼Ÿ

A: 13GBæ˜¯åŸå§‹FP16æ¨¡å‹å¤§å°ï¼ŒINT4é‡åŒ–åï¼š
- æ¨¡å‹å‚æ•°ä»16bitå‹ç¼©åˆ°4bit
- å®é™…åŠ è½½åˆ°æ˜¾å­˜çº¦4-5GB
- æ¨ç†æ—¶éœ€è¦é¢å¤–1-2GBç¼“å­˜

### Q2: å¦‚ä½•çŸ¥é“å½“å‰æ˜¾å­˜ä½¿ç”¨ï¼Ÿ

```bash
# Windows
nvidia-smi

# Python
python -c "import torch; print(torch.cuda.memory_allocated() / 1024**3)"
```

### Q3: æ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

1. å‡å°max_lengthï¼ˆ4096â†’2048ï¼‰
2. ä½¿ç”¨æ›´å°æ¨¡å‹ï¼ˆQwen2-1.5Bï¼‰
3. é™ä½å¹¶å‘æ•°
4. ä½¿ç”¨CPUï¼ˆæ…¢ä½†å¯ç”¨ï¼‰

### Q4: å¯ä»¥åŒæ—¶åŠ è½½å¤šä¸ªæ¨¡å‹å—ï¼Ÿ

A: ä¸æ¨èã€‚RTX 3070 8GBåªå¤ŸåŠ è½½1ä¸ªæ¨¡å‹ã€‚
å¯ä»¥é€šè¿‡é…ç½®åˆ‡æ¢ä¸åŒæ¨¡å‹ã€‚

---

**å¼€å§‹é€‰æ‹©é€‚åˆä½ çš„æ¨¡å‹å§ï¼** ğŸš€
