"""
ChatGLM3 LLMå®žçŽ°

çœŸå®žçš„ChatGLM3-6Bæ¨¡åž‹å°è£…
éœ€è¦GPUçŽ¯å¢ƒè¿è¡Œ
"""

import asyncio
from typing import AsyncIterator, Optional, Dict, Any
from .base import BaseLLM, LLMConfig


class ChatGLMLLM(BaseLLM):
    """ChatGLM3 LLM - çœŸå®žæ¨¡åž‹"""
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡åž‹"""
        try:
            from transformers import AutoTokenizer, AutoModel
            import torch
        except ImportError:
            raise ImportError(
                "éœ€è¦å®‰è£…transformerså’Œtorch: "
                "pip install transformers torch"
            )
        
        print(f"ðŸ¤– åŠ è½½ChatGLM3æ¨¡åž‹: {self.config.model_path}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_path,
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡åž‹
        self.model = AutoModel.from_pretrained(
            self.config.model_path,
            trust_remote_code=True
        )
        
        # é‡åŒ–
        if self.config.quantization:
            if self.config.quantization == "int4":
                self.model = self.model.quantize(4)
            elif self.config.quantization == "int8":
                self.model = self.model.quantize(8)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        if self.config.device == "cuda":
            self.model = self.model.cuda()
        
        self.model = self.model.eval()
        
        print(f"âœ… æ¨¡åž‹åŠ è½½å®Œæˆ")
        print(f"   è®¾å¤‡: {self.config.device}")
        print(f"   é‡åŒ–: {self.config.quantization or 'None'}")
    
    async def generate(
        self,
        prompt: str,
        max_length: Optional[int] = None,
        temperature: Optional[float] = None,
        **kwargs
    ) -> str:
        """ç”Ÿæˆå“åº”ï¼ˆéžæµå¼ï¼‰"""
        max_length = max_length or self.config.max_length
        temperature = temperature or self.config.temperature
        
        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼ˆé¿å…é˜»å¡žäº‹ä»¶å¾ªçŽ¯ï¼‰
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            self._generate_sync,
            prompt,
            max_length,
            temperature
        )
        
        return response
    
    def _generate_sync(
        self,
        prompt: str,
        max_length: int,
        temperature: float
    ) -> str:
        """åŒæ­¥ç”Ÿæˆ"""
        response, _ = self.model.chat(
            self.tokenizer,
            prompt,
            max_length=max_length,
            temperature=temperature,
            top_p=self.config.top_p
        )
        return response
    
    async def generate_stream(
        self,
        prompt: str,
        max_length: Optional[int] = None,
        temperature: Optional[float] = None,
        **kwargs
    ) -> AsyncIterator[str]:
        """ç”Ÿæˆå“åº”ï¼ˆæµå¼ï¼‰"""
        max_length = max_length or self.config.max_length
        temperature = temperature or self.config.temperature
        
        # ChatGLM3çš„æµå¼ç”Ÿæˆ
        for response, _ in self.model.stream_chat(
            self.tokenizer,
            prompt,
            max_length=max_length,
            temperature=temperature,
            top_p=self.config.top_p
        ):
            yield response
            await asyncio.sleep(0)  # è®©å‡ºæŽ§åˆ¶æƒ
    
    def get_info(self) -> Dict[str, Any]:
        """èŽ·å–æ¨¡åž‹ä¿¡æ¯"""
        info = {
            "model_name": self.config.name,
            "model_type": "chatglm3",
            "model_path": self.config.model_path,
            "device": self.config.device,
            "quantization": self.config.quantization,
        }
        
        # å¦‚æžœåœ¨CUDAä¸Šï¼Œæ·»åŠ æ˜¾å­˜ä¿¡æ¯
        if self.config.device == "cuda":
            try:
                import torch
                if torch.cuda.is_available():
                    info["gpu_name"] = torch.cuda.get_device_name(0)
                    info["gpu_memory_allocated"] = f"{torch.cuda.memory_allocated(0) / 1024**3:.2f} GB"
                    info["gpu_memory_reserved"] = f"{torch.cuda.memory_reserved(0) / 1024**3:.2f} GB"
            except:
                pass
        
        return info
    
    async def close(self):
        """å…³é—­æ¨¡åž‹ï¼Œé‡Šæ”¾èµ„æº"""
        if self.model is not None:
            del self.model
            self.model = None
        
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        
        # æ¸…ç†CUDAç¼“å­˜
        if self.config.device == "cuda":
            try:
                import torch
                torch.cuda.empty_cache()
            except:
                pass


# ä¾¿æ·å‡½æ•°
def create_chatglm_llm(config: LLMConfig) -> ChatGLMLLM:
    """åˆ›å»ºChatGLM LLMå®žä¾‹"""
    return ChatGLMLLM(config)
