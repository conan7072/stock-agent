"""
LLMå·¥å‚

æ ¹æ®é…ç½®è‡ªåŠ¨åˆ›å»ºMockæˆ–çœŸå®LLM
"""

from typing import Optional
from .base import BaseLLM, LLMConfig
from .mock_llm import MockLLM
from .chatglm_llm import ChatGLMLLM


def create_llm(config: Optional[LLMConfig] = None) -> BaseLLM:
    """
    åˆ›å»ºLLMå®ä¾‹
    
    æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©Mock LLMæˆ–çœŸå®LLM
    
    Args:
        config: LLMé…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        
    Returns:
        LLMå®ä¾‹
    """
    if config is None:
        config = LLMConfig()
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Mockæ¨¡å¼
    if config.mock_mode:
        print("ğŸ­ ä½¿ç”¨Mock LLMï¼ˆæ— éœ€GPUï¼‰")
        return MockLLM(config)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰GPU
    try:
        import torch
        has_cuda = torch.cuda.is_available()
        
        if config.device == "cuda" and not has_cuda:
            print("âš ï¸  æœªæ£€æµ‹åˆ°CUDAï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°Mockæ¨¡å¼")
            config.mock_mode = True
            return MockLLM(config)
    except ImportError:
        print("âš ï¸  æœªå®‰è£…PyTorchï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°Mockæ¨¡å¼")
        config.mock_mode = True
        return MockLLM(config)
    
    # ä½¿ç”¨çœŸå®æ¨¡å‹
    print(f"ğŸ¤– ä½¿ç”¨çœŸå®LLM: {config.name}")
    
    if "chatglm" in config.name.lower():
        return ChatGLMLLM(config)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {config.name}")


def create_llm_from_config_file(config_path: str) -> BaseLLM:
    """
    ä»é…ç½®æ–‡ä»¶åˆ›å»ºLLM
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆYAMLï¼‰
        
    Returns:
        LLMå®ä¾‹
    """
    import yaml
    from pathlib import Path
    
    config_file = Path(config_path)
    
    if not config_file.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_file, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)
    
    # æå–modelé…ç½®
    model_config = config_dict.get('model', {})
    
    # åˆ›å»ºLLMConfig
    llm_config = LLMConfig(
        name=model_config.get('name', 'chatglm3-6b'),
        model_path=model_config.get('path', './models/chatglm3-6b'),
        device=model_config.get('device', 'cuda'),
        quantization=model_config.get('quantization', 'int4'),
        max_length=model_config.get('max_length', 4096),
        temperature=model_config.get('temperature', 0.7),
        top_p=model_config.get('top_p', 0.9),
        mock_mode=model_config.get('mock_mode', False)
    )
    
    return create_llm(llm_config)


# å…¨å±€LLMå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_global_llm: Optional[BaseLLM] = None


def get_llm() -> BaseLLM:
    """
    è·å–å…¨å±€LLMå®ä¾‹
    
    å¦‚æœè¿˜æœªåˆå§‹åŒ–ï¼Œåˆ™ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»º
    
    Returns:
        LLMå®ä¾‹
    """
    global _global_llm
    
    if _global_llm is None:
        _global_llm = create_llm()
    
    return _global_llm


def set_llm(llm: BaseLLM):
    """
    è®¾ç½®å…¨å±€LLMå®ä¾‹
    
    Args:
        llm: LLMå®ä¾‹
    """
    global _global_llm
    _global_llm = llm


async def close_llm():
    """å…³é—­å…¨å±€LLMå®ä¾‹"""
    global _global_llm
    
    if _global_llm is not None:
        await _global_llm.close()
        _global_llm = None
